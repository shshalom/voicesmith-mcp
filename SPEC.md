# Agent Voice MCP Server

A Model Context Protocol (MCP) server that provides local, high-quality text-to-speech **and** speech-to-text capabilities to AI coding assistants. Built on Kokoro ONNX (TTS) and faster-whisper (STT) for fast, fully offline voice interaction â€” enabling AI agents to speak with distinct voices and listen to user responses during development sessions.

Works with Claude Code, Cursor, VS Code, Windsurf, Zed, JetBrains, and any other tool that supports the MCP standard.

---

## Background

AI coding assistants are text-only by default. This project adds a full voice layer so that:
- The main AI agent speaks responses aloud (bookend pattern: opening + closing voice)
- Sub-agents and team agents each have distinct voices, creating a "team of people" feel
- Handoffs between agents are audible â€” you hear who's talking without reading
- The AI can listen to your spoken responses â€” no typing needed when asked a question
- All synthesis and transcription runs **locally** with zero network dependency and sub-second latency

### Why not cloud TTS/STT?

We started with Microsoft Edge TTS (cloud-based) and experienced 15-20 second latency per voice call due to network round-trips. Switching to Kokoro ONNX (local TTS) reduced this to under 1 second on Apple Silicon. For STT, faster-whisper runs the same Whisper model locally with no API key or subscription. The MCP server keeps both models loaded in memory, eliminating per-call overhead.

---

## Architecture

### Overview

```
AI Assistant (Claude Code, Cursor, etc.)
    â”‚
    â”‚  MCP tool calls
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Agent Voice MCP Server                   â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   TTS Engine        â”‚   â”‚   STT Engine            â”‚ â”‚
â”‚  â”‚                     â”‚   â”‚                         â”‚ â”‚
â”‚  â”‚  Kokoro ONNX (82M)  â”‚   â”‚  faster-whisper (150M)  â”‚ â”‚
â”‚  â”‚  54 voices           â”‚   â”‚  Silero VAD (2MB)       â”‚ â”‚
â”‚  â”‚  loaded at startup   â”‚   â”‚  loaded at startup      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Voice Registry     â”‚   â”‚  Audio I/O              â”‚ â”‚
â”‚  â”‚  (auto-discovery)   â”‚   â”‚  Mic capture + Playback â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  Speech Queue       â”‚   Memory: ~450MB total       â”‚
â”‚  â”‚  (prevents overlap) â”‚   (300MB TTS + 150MB STT)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transport

The server uses **stdio** transport (JSON-RPC over stdin/stdout). This is the standard for Claude Code and most MCP clients. Each IDE session spawns its own server process. Two concurrent sessions = two separate processes (~450MB each), with independent registries and queues. They do not collide.

### Logging

Server logs to **stderr** (MCP convention â€” stdout is reserved for the protocol). Optionally logs to `~/.local/share/agent-voice-mcp/server.log` when `"log_file": true` is set in config. Log levels: `debug`, `info`, `warn`, `error` (configurable via `"log_level"` in config, default: `info`).

### Platform Support

- **macOS** â€” Fully supported. Apple Silicon recommended for best performance.
- **Linux** â€” Supported. x86_64 and ARM64.
- **Windows** â€” Not supported in v1. Planned for v2.

### Key Design Decisions

1. **Persistent process** â€” Both Kokoro TTS and faster-whisper STT load once at startup (~2.2s total), then stay in memory (~450MB). All subsequent calls skip model loading entirely.

2. **Internal speech queue** â€” TTS requests are serialized automatically. No file-based locking (talking stick), no race conditions, no stale locks. If two agents call `speak` simultaneously, the second waits for the first to finish.

3. **Auto-discovery voice registry** â€” When an agent calls `speak` with a name the server hasn't seen before, it automatically assigns an unused voice from the pool and registers it. No pre-configuration needed. The assignment algorithm:
   - First, check if the name matches a Kokoro voice name (e.g., agent "Eric" â†’ `am_eric`, agent "Nova" â†’ `af_nova`)
   - If no name match, pick a deterministic but unique voice from the unassigned pool (using a hash of the agent name)
   - The assigned voice persists for the server's lifetime and is **auto-saved to config.json** on graceful shutdown and periodically (every 60s), so assignments survive restarts
   - Users can optionally pre-configure mappings in `config.json` for guaranteed persistence
   - **Pool exhaustion:** When all 54 voices are assigned, new agents get a hash-based voice from the full pool (may share with an existing agent). A warning is logged: "All voices assigned, reusing voices."

4. **Configurable main agent voice** â€” The main agent's voice is not hardcoded. Users set it via:
   - `config.json` (`"main_agent"` field)
   - The `set_voice` tool at runtime
   - Or simply by calling `speak` with any name â€” the first agent to speak becomes the "main" unless configured otherwise

5. **Voice state: tool presence = voice on** â€” When the MCP server is running, voice tools are available and the AI uses them. When the server is not running, the tools don't exist and the AI falls back to text. No on/off flag to manage. Additionally, `mute`/`unmute` tools allow temporarily silencing audio without stopping the server.

6. **Voice Activity Detection (VAD)** â€” The `listen` tool uses Silero VAD (2MB ONNX model, runs on ONNX Runtime â€” no PyTorch dependency) to detect when the user stops speaking. After 1.5 seconds of silence, recording stops and transcription begins. No manual "stop" action needed. The VAD requires a 64-sample context window prepended to each 512-sample chunk for accurate detection.

7. **Temp file auto-cleanup** â€” Audio files are generated to a temp path, played, and immediately deleted. No accumulation.

8. **Cross-platform audio** â€” Uses `mpv` for playback (macOS, Linux). Falls back to system commands (`afplay` on macOS, `aplay` on Linux) if mpv is unavailable.

---

## MCP Tools

### TTS Tools (Voice Output)

#### `speak`

Synthesize and play speech for a named agent.

**Parameters:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| `name` | string | Yes | Agent name (e.g., "Eric", "Nova"). Maps to a voice via the registry. Unknown names are **auto-assigned** a unique voice from the available pool and registered for future calls. |
| `text` | string | Yes | The text to speak. |
| `speed` | number | No | Speech speed multiplier. Default: 1.0 |
| `block` | boolean | No | Whether to wait for playback to complete before returning. Default: true |

**Returns (block: true):**
```json
{
  "success": true,
  "voice": "am_eric",
  "auto_assigned": false,
  "duration_ms": 850,
  "synthesis_ms": 420
}
```

**Returns (block: false):** Fire-and-forget. Returns immediately after queuing (before synthesis or playback). No callback/poll mechanism â€” intended for the opening voice in the bookend pattern where the AI doesn't need to wait.
```json
{
  "success": true,
  "voice": "am_eric",
  "auto_assigned": false,
  "queued": true
}
```

`auto_assigned` is `true` when the server assigned a new voice for a previously unknown agent name.

**Text handling:** Plain text only â€” no SSML or markup supported. For long text (>500 characters), the server **auto-chunks** by sentence (splits on `.` `!` `?`), synthesizes each chunk, and plays them sequentially with no gap. No hard rejection or length limit â€” just auto-chunking. This also enables future streaming playback (play chunk 1 while synthesizing chunk 2).

#### `list_voices`

List all available Kokoro voices.

**Parameters:** None

**Returns:**
```json
{
  "voices": [
    { "id": "am_eric", "gender": "male", "accent": "american" },
    { "id": "af_nova", "gender": "female", "accent": "american" },
    ...
  ],
  "total": 54
}
```

#### `get_voice_registry`

Get current agent-to-voice mappings. This registry is built dynamically â€” entries are added as agents speak for the first time, or via `set_voice`.

**Parameters:** None

**Returns:**
```json
{
  "registry": {
    "default": "am_eric",
    "Eric": "am_eric",
    "Fenrir": "am_fenrir"
  },
  "available_pool": ["af_nova", "am_onyx", "af_heart", "am_adam", "..."],
  "total_assigned": 2,
  "total_available": 51
}
```

#### `set_voice`

Assign or reassign a voice to an agent name.

**Parameters:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| `name` | string | Yes | Agent name to assign. |
| `voice` | string | Yes | Kokoro voice ID (e.g., "am_eric"). Must be a valid voice from `list_voices`. |

**Returns:**
```json
{
  "success": true,
  "name": "Eric",
  "voice": "am_eric"
}
```

**Invalid voice ID:** If the voice ID doesn't exist in the 53 available voices, returns `{ "success": false, "error": "invalid_voice", "message": "Voice 'am_xyz' not found. Use list_voices to see available options." }`

#### `stop`

Stop any currently playing audio **and** cancel any active `listen` recording. If a `listen` is in progress, it returns `{ "success": false, "cancelled": true }` to the caller.

**Parameters:** None

**Returns:**
```json
{
  "success": true,
  "stopped_playback": true,
  "cancelled_listen": false
}
```

#### `mute`

Temporarily silence all voice output. The `speak` tool still returns success but no audio plays. Useful when in a meeting or shared space.

**Parameters:** None

**Returns:**
```json
{
  "success": true,
  "muted": true
}
```

#### `unmute`

Resume voice output after muting.

**Parameters:** None

**Returns:**
```json
{
  "success": true,
  "muted": false
}
```

### Combined Tools

#### `speak_then_listen`

Convenience tool that speaks a question and immediately listens for the answer in one atomic call. Reduces two MCP round-trips to one â€” ideal for the "ask a question and wait for voice response" pattern.

**Parameters:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| `name` | string | Yes | Agent name for the voice. |
| `text` | string | Yes | The question to speak. |
| `speed` | number | No | Speech speed. Default: 1.0 |
| `timeout` | number | No | Max seconds to wait for response. Default: 15 |
| `silence_threshold` | number | No | Seconds of silence before stopping. Default: 1.5 |

**Returns:**
```json
{
  "speak": { "success": true, "voice": "am_eric", "duration_ms": 1200 },
  "listen": { "success": true, "text": "Go with REST", "confidence": 0.96 }
}
```

### Diagnostic Tools

#### `status`

Report server health and component status. Always available, even if TTS or STT failed to load.

**Parameters:** None

**Returns:**
```json
{
  "tts": { "loaded": true, "model": "kokoro-v1.0.onnx", "voices": 54 },
  "stt": { "loaded": true, "model": "whisper-base", "language": "en" },
  "vad": { "loaded": true },
  "muted": false,
  "uptime_s": 3600,
  "registry_size": 3,
  "queue_depth": 0
}
```

### STT Tools (Voice Input)

#### `listen`

Activate the microphone, record the user's speech, and return the transcribed text. Uses Silero VAD to automatically detect when the user stops speaking (1.5s silence threshold).

This is a **blocking** tool call â€” the AI waits while the user speaks, then receives the transcribed text as the tool result. No keyboard input needed from the user.

**Parameters:**
| Name | Type | Required | Description |
|------|------|----------|-------------|
| `timeout` | number | No | Maximum seconds to wait for speech. Default: 15 |
| `prompt` | string | No | Optional context about what the AI is asking (for logging/display). |
| `silence_threshold` | number | No | Seconds of silence before stopping. Default: 1.5 |

**Returns:**
```json
{
  "success": true,
  "text": "Go with REST",
  "confidence": 0.96,
  "duration_ms": 2100,
  "transcription_ms": 200
}
```

**Behavior:**
1. Mic activates (user sees ğŸ™ï¸ indicator)
2. Silero VAD monitors for speech
3. User speaks naturally
4. VAD detects 1.5s of silence â†’ recording stops
5. faster-whisper transcribes the audio (~0.2s)
6. Text returned to AI as tool result
7. If timeout reached with no speech â†’ returns `{ "success": false, "error": "timeout" }`
8. If cancelled via MCP cancellation notification or `stop` tool â†’ returns `{ "success": false, "cancelled": true }`
9. If muted â†’ returns immediately: `{ "success": false, "error": "muted", "message": "Voice input is muted" }`

**Cancellation:** The server is a stdio subprocess with no TTY access â€” it cannot detect keystrokes directly. Cancellation is handled through two mechanisms:
1. **MCP protocol cancellation** â€” The client sends a `notifications/cancelled` message for the in-progress `listen` request. The server respects this and stops recording immediately.
2. **The `stop` tool** â€” Another tool call that interrupts any active `listen` (already specified above).

The "press Escape to cancel" experience is a **client-side UX concern** â€” the IDE detects Escape and sends one of the above signals. The server itself never sees keystrokes.

**Concurrent calls:** Only one `listen` can be active at a time (one microphone). If a second `listen` is called while one is in progress, it returns immediately: `{ "success": false, "error": "mic_busy", "message": "Another listen call is in progress" }`. The first call is not interrupted.

**Fallback:** If `listen` returns cancelled or timeout, the AI should not re-call `listen`. Instead, fall back to requesting text input.

---

## Voice State Management

### Tool Presence = Voice On

The primary on/off mechanism is simply whether the MCP server is running:

- **Server running** â†’ `speak`, `listen`, and all voice tools are available â†’ AI uses voice
- **Server not running** â†’ tools don't exist in the AI's toolbox â†’ AI uses text only

No configuration flags or toggles needed. The CLAUDE.md voice rules instruct the AI: "If the `speak` tool is available, use voice. If not, use text."

### Mute/Unmute (Temporary Silence)

When the server IS running but you want to temporarily silence it:

- `mute()` â†’ `speak` calls return success silently (no audio). `listen` is also disabled.
- `unmute()` â†’ normal voice resumes.

| Scenario | What happens |
|----------|-------------|
| Server off | No voice tools available â†’ text only |
| Server on | Voice works normally |
| Server on + muted | `speak` returns success but no audio plays |
| Server on + unmuted | Back to normal voice |

---

## STT Architecture

### Engine: faster-whisper

| Property | Value |
|----------|-------|
| Model | OpenAI Whisper (open source, MIT license) |
| Runtime | CTranslate2 (optimized inference) |
| Speed | ~4x faster than real-time on Apple Silicon |
| Accuracy | Excellent (same as OpenAI Whisper API) |
| Model size | base: 150MB, small: 500MB, medium: 1.5GB |
| Languages | 99 |
| Network | None (fully local) |
| Cost | Free |

### Confidence Score Computation

The `confidence` field in `listen` responses is computed as `exp(avg_logprob)` where `avg_logprob` is the average log probability across all segments returned by faster-whisper (note: the attribute is `avg_logprob`, not `avg_log_prob`). For single-segment transcriptions (typical for short commands), this is the direct segment probability. Range: 0.0 to 1.0.

### Voice Activity Detection: Silero VAD (ONNX)

| Property | Value |
|----------|-------|
| Size | 2MB |
| Type | Neural network (ONNX) |
| Runtime | ONNX Runtime (shared with Kokoro TTS â€” no PyTorch dependency) |
| Latency | ~30ms |
| What it detects | Speech vs silence, ignores keyboard typing, fan noise, coughs |
| Silence threshold | 1.5s (configurable) |
| Chunk size | 512 samples at 16kHz (32ms) + 64-sample context window |

### How `listen` Works Internally

```
AI calls listen(timeout=15)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Activate microphone          â”‚
â”‚     Show ğŸ™ï¸ indicator           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. Silero VAD monitors audio    â”‚
â”‚     Waiting for speech...        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. Speech detected              â”‚
â”‚     Recording audio buffer       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. 1.5s silence detected        â”‚
â”‚     Stop recording               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. faster-whisper transcribes   â”‚
â”‚     (~0.2s for short commands)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  6. Return text to AI            â”‚
â”‚     { "text": "Go with REST" }   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### User-Initiated Voice Input (Push-to-Talk) â€” v2 / Client-Side

> **Note:** This is a client-side feature, not part of the MCP server. It depends on IDE-specific hooks (Claude Code hooks, Cursor extensions, etc.) and is planned for v2.

For when the user wants to speak without the AI asking:

```
User types: /voice (or presses configured hotkey)
â†’ Client-side hook calls the MCP listen tool
â†’ User speaks
â†’ VAD detects silence â†’ transcribe
â†’ Text auto-submitted to AI as input (via AppleScript/keystroke injection)
```

This bypasses the terminal's Enter key requirement â€” text is injected and submitted programmatically. Implementation details are IDE-specific and out of scope for the MCP server itself.

---

## Full Conversation Flow Example

```
User types: "Add webhook support"

ğŸ”Š Eric: "Got it, working on it."
   (AI works, spawns architect agent)

ğŸ”Š Fenrir: "We'll need a POST endpoint with HMAC validation."
   (AI spawns explorer)

ğŸ”Š Nova: "Found the existing route patterns."

ğŸ”Š Fenrir: "Onyx, scaffold the route."

ğŸ”Š Onyx: "On it... Done. Ready for review."

ğŸ”Š Eric: "Looks good. Should I add rate limiting too?"
   AI calls: listen(timeout=15, prompt="Add rate limiting?")
   ğŸ™ï¸ Mic ON
   User speaks: "Yeah, add rate limiting"
   ğŸ™ï¸ Mic OFF â†’ transcribe â†’ "Yeah, add rate limiting"

ğŸ”Š Eric: "Adding rate limiting now."
   (AI works...)

ğŸ”Š Eric: "All done. Webhook with rate limiting is ready."
```

No typing needed for the user's response. No Enter key. Fully hands-free when the AI asks questions.

---

## Configuration

### MCP Config (per IDE)

The installer writes the MCP server entry to the correct config file for each selected IDE:

| IDE | Config Path |
|-----|------------|
| **Claude Code** | `~/.claude.json` |
| **Cursor** | `~/.cursor/mcp.json` |
| **Codex (OpenAI)** | `~/.codex/mcp.json` |

All use the same server entry format:
```json
{
  "mcpServers": {
    "agent-voice": {
      "command": "~/.local/share/agent-voice-mcp/.venv/bin/python3",
      "args": ["~/.local/share/agent-voice-mcp/server.py"]
    }
  }
}
```

> **Note:** The installer writes the actual venv Python path during setup. Use `--claude`, `--cursor`, `--codex`, or `--all` flags to target specific IDEs, or let the installer auto-detect installed IDEs.

### Server Configuration File (`config.json`)

**Runtime location:** `~/.local/share/agent-voice-mcp/config.json` (created during install)
**Lookup order:** `$AGENT_VOICE_CONFIG` env var â†’ `~/.local/share/agent-voice-mcp/config.json` â†’ built-in defaults
The `config.json` in the project repo root is the **default template** copied during install.

```json
{
  "tts": {
    "model_path": "~/.local/share/agent-voice-mcp/models/kokoro-v1.0.onnx",
    "voices_path": "~/.local/share/agent-voice-mcp/models/voices-v1.0.bin",
    "default_voice": "am_eric",
    "default_speed": 1.0,
    "audio_player": "mpv"
  },
  "stt": {
    "model_size": "base",
    "language": "en",
    "silence_threshold": 1.5,
    "max_listen_timeout": 15
  },
  "main_agent": "Eric",
  "voice_registry": {},
  "log_level": "info",
  "log_file": false
}
```

The `voice_registry` is **optional** â€” it starts empty by default. Voices are auto-assigned as agents speak. Users can pre-populate it to persist specific assignments across sessions, or to pin favorite voices to agent names.

The `main_agent` field identifies which agent name is the primary/lead agent. This is informational â€” it does not restrict other agents from speaking.

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `AGENT_VOICE_CONFIG` | Override path to config.json | `~/.local/share/agent-voice-mcp/config.json` |
| `KOKORO_MODEL` | Path to kokoro-v1.0.onnx model file | `~/.local/share/agent-voice-mcp/models/kokoro-v1.0.onnx` |
| `KOKORO_VOICES` | Path to voices-v1.0.bin file | `~/.local/share/agent-voice-mcp/models/voices-v1.0.bin` |
| `WHISPER_MODEL` | faster-whisper model size | `base` |
| `VOICE_PLAYER` | Audio player command | `mpv` |
| `VOICE_DEFAULT` | Default voice ID | `am_eric` |

---

## Voice Registry

### Auto-Discovery & Assignment

The voice registry is **dynamic**. No pre-configuration is required.

**When a new agent name calls `speak` for the first time:**

1. **Name matching** â€” If the agent name matches a Kokoro voice name (case-insensitive), that voice is assigned automatically. Examples:
   - Agent "Eric" â†’ `am_eric`
   - Agent "Nova" â†’ `af_nova`
   - Agent "Bella" â†’ `af_bella`
   - Agent "George" â†’ `bm_george`

2. **Hash-based assignment** â€” If no name match is found, a deterministic hash of the agent name selects from the unassigned voice pool. This ensures the same agent name always gets the same voice, even across restarts (if not pre-configured).

3. **Override anytime** â€” Users or agents can call `set_voice` to change any assignment.

### Main Agent Voice

The main agent's voice is **not hardcoded**. Users choose it via:
- `config.json` â†’ `"default_voice": "am_eric"` (persists across sessions)
- `set_voice` tool at runtime (session-only unless saved to config)

### All Available Voices (54 total)

**American English (20):**
- Female (11): af_alloy, af_aoede, af_bella, af_heart, af_jessica, af_kore, af_nicole, af_nova, af_river, af_sarah, af_sky
- Male (9): am_adam, am_echo, am_eric, am_fenrir, am_liam, am_michael, am_onyx, am_puck, am_santa

**British English (8):**
- Female (4): bf_alice, bf_emma, bf_isabella, bf_lily
- Male (4): bm_daniel, bm_fable, bm_george, bm_lewis

**Other Languages (26):**
- Spanish: ef_dora, em_alex, em_santa
- French: ff_siwis
- Hindi: hf_alpha, hf_beta, hm_omega, hm_psi
- Italian: if_sara, im_nicola
- Japanese: jf_alpha, jf_gongitsune, jf_nezumi, jf_tebukuro, jm_kumo
- Portuguese: pf_dora, pm_alex, pm_santa
- Mandarin: zf_xiaobei, zf_xiaoni, zf_xiaoxiao, zf_xiaoyi, zm_yunjian, zm_yunxi, zm_yunxia, zm_yunyang

---

## Voice Behavior Rules (IDE-Specific Injection)

The MCP server handles the **mechanics** (synthesize, play, record, transcribe). The **behavioral rules** â€” when the AI should speak, the bookend pattern, sub-agent voice assignment â€” are injected directly into each IDE's instruction file during install.

### How It Works

The installer asks the user to pick a main agent voice (e.g., "Eric" â†’ `am_eric`), then writes **personalized** behavior rules into the IDE's config:

| IDE | Rules Location | Method |
|-----|---------------|--------|
| **Claude Code** | `~/.claude/CLAUDE.md` | Appended block with sentinel comment |
| **Cursor** | `~/.cursor/rules/agent-voice.mdc` | Standalone MDC file with `alwaysApply: true` |
| **Codex (OpenAI)** | `~/.codex/AGENTS.md` | Appended block with sentinel comment |

All injected blocks are marked with `<!-- installed by agent-voice-mcp -->` for idempotent updates and clean uninstall.

### Rules Content

The rules are personalized with the chosen main agent name and teach the AI:

1. **Voice identity** â€” "You are **Eric**. Always call `speak` with `name: "Eric"`."
2. **Bookend speaking pattern** â€” Opening voice (`block: false`) + closing voice (`block: true`, never skip)
3. **Mandatory voice for questions** â€” Whenever asking the user a question, MUST speak it aloud using `speak_then_listen` or `speak`
4. **Sub-agent voice assignment** â€” Call `get_voice_registry` to see available voices, pick names matching Kokoro voices, never reuse the main agent's name
5. **Handoff protocol** â€” Both agents speak on handoffs
6. **Fallback** â€” Text-only when tools unavailable, respect muted state

### Template

The raw template is at `templates/voice-rules.md` with `{{MAIN_AGENT}}` placeholders. The installer fills these in per the user's voice choice.

---

## Dependencies

### Runtime
- **Python 3.11+** (3.11 or 3.12 recommended; 3.13/3.14 have compatibility issues with some deps)
- **kokoro-onnx** â€” Kokoro TTS with ONNX Runtime (no PyTorch needed)
- **faster-whisper** â€” Whisper STT with CTranslate2 optimization
- **silero-vad** â€” Voice Activity Detection via ONNX Runtime (no PyTorch â€” uses the same ONNX Runtime as Kokoro TTS)
- **soundfile** â€” Audio file I/O
- **sounddevice** â€” Microphone capture
- **mcp** â€” MCP Python SDK
- **mpv** â€” Audio playback (or system alternative)
- **espeak-ng** â€” Phonemizer backend (required by Kokoro)

### Model Files (downloaded during install)
- `kokoro-v1.0.onnx` â€” 310MB (TTS model)
- `voices-v1.0.bin` â€” 27MB (TTS voice embeddings)
- `whisper-base` â€” ~150MB (STT model, auto-downloaded by faster-whisper)
- `silero_vad.onnx` â€” ~2MB (VAD model)

### System Requirements
- macOS (Apple Silicon recommended) or Linux
- ~450MB RAM for loaded models (300MB TTS + 150MB STT)
- ~500MB disk for model files
- Microphone access (for STT)

---

## Installation

### Recommended: npx (one command, no cloning)

```bash
npx agent-voice-mcp install              # Auto-detect installed IDEs
npx agent-voice-mcp install --claude     # Claude Code only
npx agent-voice-mcp install --cursor     # Cursor only
npx agent-voice-mcp install --codex      # Codex (OpenAI) only
npx agent-voice-mcp install --all        # All supported IDEs
```

Run from anywhere â€” no need to clone a repo or navigate to a folder. The installer detects existing tools and models, skips what's already installed, and only downloads/installs what's missing.

### What the installer does

```
ğŸ™ï¸  Agent Voice MCP â€” Local AI Voice System

Step 1/6: Checking system dependencies...
  âœ“ Python 3.12 found
  âœ“ espeak-ng found
  âœ“ mpv found

Step 2/6: Setting up Python environment...
  âœ“ Server files copied to ~/.local/share/agent-voice-mcp
  âœ“ Created venv at ~/.local/share/agent-voice-mcp/.venv
  âœ“ All packages installed

Step 3/6: Checking models...
  âœ“ kokoro-v1.0.onnx already installed (or downloaded / symlinked from existing install)
  âœ“ voices-v1.0.bin already installed
  â„¹ whisper-base model (~150MB) will download automatically on first use

Step 4/6: Configuring MCP server...
  â„¹ Detected: Claude Code, Cursor
  âœ“ Claude Code: already configured
  âœ“ Cursor: added to ~/.cursor/mcp.json

Step 5/6: Checking microphone access...
  âœ“ Microphone access granted

Step 6/6: Setting up voice rules...
  Choose your main agent voice:
    1) Eric (am_eric â€” male, American, confident)
    2) Nova (af_nova â€” female, American, clear)
    ...
  âœ“ Main agent voice: Eric (am_eric)
  âœ“ Claude Code: voice rules added to ~/.claude/CLAUDE.md
  âœ“ Cursor: voice rules written to ~/.cursor/rules/agent-voice.mdc

ğŸ‰ Done! Configured for: Claude Code, Cursor
   Restart your IDE session, then voice tools will be available.
   Run "npx agent-voice-mcp test" to hear a sample voice.
```

**Smart detection:** Re-running the installer shows all green checkmarks â€” it's fully idempotent. It also detects existing Kokoro model files (e.g., from `~/.local/share/kokoro-tts/models/`) and symlinks them instead of re-downloading.

### Other npx commands

```bash
npx agent-voice-mcp install      # Full interactive setup
npx agent-voice-mcp test         # Play a sample voice to verify
npx agent-voice-mcp voices       # Browse and preview all 54 voices
npx agent-voice-mcp config       # Re-run voice picker / change settings
npx agent-voice-mcp uninstall    # Remove everything cleanly
```

### Uninstall

```bash
npx agent-voice-mcp uninstall
```

Prompts for confirmation, then removes:
- Python venv at `~/.local/share/agent-voice-mcp/`
- Models at `~/.local/share/agent-voice-mcp/models/`
- Config at `~/.local/share/agent-voice-mcp/config.json`
- Server log at `~/.local/share/agent-voice-mcp/server.log`
- MCP server entries from all configured IDEs (Claude Code, Cursor, Codex)
- Voice rules blocks from all IDE instruction files
- Voice rules template at `~/.claude/agents/voice-rules.md`

Does **not** remove: npm cache (managed by npm).

### Alternative: Git clone (for contributors)

```bash
git clone https://github.com/<user>/agent-voice-mcp.git
cd agent-voice-mcp
./install.sh
```

### Manual Install (power users)

```bash
# 1. Install system deps
brew install espeak-ng mpv  # macOS
# apt install espeak-ng mpv  # Linux

# 2. Create venv and install Python deps
python3.12 -m venv .venv
source .venv/bin/activate
pip install kokoro-onnx faster-whisper silero-vad soundfile sounddevice mcp

# 3. Download TTS model files
mkdir -p models
curl -L -o models/kokoro-v1.0.onnx https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx
curl -L -o models/voices-v1.0.bin https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin

# 4. Test
python server.py --test
```

---

## Project Structure

```
agent-voice-mcp/
â”œâ”€â”€ SPEC.md                # This file
â”œâ”€â”€ README.md              # User-facing documentation
â”œâ”€â”€ LICENSE                # Apache 2.0
â”œâ”€â”€ package.json           # npm package (for npx agent-voice-mcp)
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ cli.js             # npx entry point â€” command router
â”‚   â”œâ”€â”€ install.js         # 6-step interactive installer
â”‚   â”œâ”€â”€ uninstall.js       # Clean removal with confirmation
â”‚   â”œâ”€â”€ test-voice.js      # Quick smoke test
â”‚   â”œâ”€â”€ voices.js          # Browse and preview all voices
â”‚   â”œâ”€â”€ config.js          # Re-run voice picker / change settings
â”‚   â””â”€â”€ utils.js           # Shared helpers (Python discovery, IDE configs, logging)
â”œâ”€â”€ install.sh             # Alternative setup script (for git clone)
â”œâ”€â”€ config.json            # Default server configuration
â”œâ”€â”€ server.py              # MCP server entry point (10 tools via FastMCP)
â”œâ”€â”€ shared.py              # Constants, voice catalog, types, exceptions
â”œâ”€â”€ config.py              # Configuration management (load/save with env overrides)
â”œâ”€â”€ voice_registry.py      # Auto-discovery voice registry (name matching + hash)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ tts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kokoro_engine.py   # Kokoro ONNX wrapper (model loading, synthesis)
â”‚   â”œâ”€â”€ speech_queue.py    # Sequential speech queue (prevents overlap)
â”‚   â””â”€â”€ audio_player.py    # Audio playback (mpv, afplay, aplay)
â”œâ”€â”€ stt/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ whisper_engine.py  # faster-whisper wrapper (model loading, transcription)
â”‚   â”œâ”€â”€ mic_capture.py     # Microphone recording with sounddevice (blocksize=512)
â”‚   â””â”€â”€ vad.py             # Silero VAD via ONNX Runtime (no PyTorch)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ voice-rules.md     # Voice behavior template ({{MAIN_AGENT}} placeholder)
â”œâ”€â”€ models/                # Model files (downloaded during install)
â”‚   â”œâ”€â”€ kokoro-v1.0.onnx
â”‚   â””â”€â”€ voices-v1.0.bin
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py        # Shared mock fixtures
    â”œâ”€â”€ test_tts.py        # 51 TTS tests
    â”œâ”€â”€ test_stt.py        # 23 STT tests
    â”œâ”€â”€ test_registry.py   # 20 registry tests
    â””â”€â”€ test_server.py     # 26 integration tests
```

---

## Testing

### Framework

**pytest** with optional model mocking for CI environments.

### Running Tests

```bash
# Activate venv
source .venv/bin/activate

# Run all tests
pytest tests/

# Run with verbose output
pytest tests/ -v

# Run a specific test file
pytest tests/test_tts.py
```

### Test Coverage

| Test file | What it covers |
|-----------|---------------|
| `test_tts.py` | Kokoro engine loading, synthesis output format, voice selection, speed control, auto-chunking of long text, temp file cleanup |
| `test_stt.py` | Whisper engine loading, transcription accuracy, VAD silence detection timing, confidence score computation, timeout behavior |
| `test_registry.py` | Name matching (exact + case-insensitive), hash-based assignment, pool exhaustion + wrap-around, persistence (save/load config.json), set_voice override |
| `test_server.py` | MCP tool call routing, speak block/non-block modes, mute/unmute state transitions, concurrent listen rejection, stop cancelling listen, speak_then_listen atomic flow, status tool output, graceful degradation when engines fail |

### CI

GitHub Actions with a macOS runner for Apple Silicon testing. Tests can run **without models** by mocking the TTS/STT engines â€” this keeps CI fast and avoids downloading 490MB of models on every run.

---

## Performance

Benchmarked on Apple M4 Max (64GB RAM):

### TTS Performance

| Metric | Shell Script (old) | MCP Server |
|--------|-------------------|------------|
| Model load | Every call (~1.3s) | Once at startup (0s after) |
| Synthesis (short) | ~0.8s | ~0.8s |
| Synthesis (medium) | ~0.9s | ~0.9s |
| Synthesis (long) | ~1.8s | ~1.8s |
| Total per call | ~2.1s | **~0.85s** |
| Memory usage | Transient | ~300MB persistent |
| Network required | No | No |
| Concurrent handling | Talking stick (file lock) | Internal queue |

Real-time factor: 0.12x (generates audio 8x faster than real-time)

### STT Performance (estimated)

| Metric | Value |
|--------|-------|
| Model load | Once at startup (~0.8s) |
| Transcription (5-word command) | ~0.2s |
| Transcription (15-word sentence) | ~0.4s |
| VAD latency | ~30ms |
| Silence detection | 1.5s (configurable) |
| Total listen-to-text | ~2.0s (1.5s silence + 0.2s transcribe + overhead) |
| Memory | ~150MB persistent |

### Server Startup

| Component | Load Time | Memory |
|-----------|-----------|--------|
| Kokoro TTS | ~1.3s | ~300MB |
| faster-whisper | ~0.8s | ~150MB |
| Silero VAD | ~0.1s | ~2MB |
| **Total** | **~2.2s** | **~450MB** |

---

## Comparison with Alternatives

| Feature | Agent Voice MCP | Edge TTS | macOS `say` | OpenAI TTS MCP |
|---------|----------------|----------|-------------|----------------|
| Runs locally | Yes | No (cloud) | Yes | No (cloud) |
| TTS latency | <1s | 15-20s | Instant | 1-3s |
| STT included | Yes | No | No | No |
| Voice quality | Very good | Excellent | Medium | Excellent |
| Voices | 54 | 322 | ~20 | 6 |
| Multi-language | 8 languages | 74 languages | English-focused | ~50 languages |
| Cost | Free | Free (unofficial) | Free | Paid (API) |
| Voice cloning | No | No | No | No |
| Emotional speech | No | No | No | No |
| MCP native | Yes | No | No | Yes |
| Network dependency | None | Always | None | Always |
| Voice input | Yes (STT) | No | No | No |

---

## Startup & Error Handling

### Graceful Degradation

If a component fails to load at startup, the server degrades gracefully instead of refusing to start:

| Failure | Behavior |
|---------|----------|
| TTS fails (bad model path, etc.) | Server starts with STT tools only. `speak` returns error. `status` reports TTS as failed with diagnostics. |
| STT fails (missing model, etc.) | Server starts with TTS tools only. `listen` returns error. `status` reports STT as failed. |
| Both fail | Server refuses to start. Exits with clear error message to stderr including paths checked and versions found. |
| VAD fails | STT starts without VAD. `listen` uses a simple energy-based silence detector as fallback. |

The `status` tool is always available and reports the health of each component, making it easy to diagnose issues.

### Graceful Shutdown

The server saves the voice registry to config.json and exits cleanly on:
- **stdin EOF** â€” MCP client disconnects (normal session end)
- **SIGTERM** â€” Process terminated by the system or IDE
- **SIGINT** â€” Ctrl+C during manual testing

All three trigger: stop any active playback/recording â†’ save registry â†’ exit 0.

### Python Discovery (npx installer)

The `cli.js` npm entry point locates a compatible Python in this order:

1. `python3.12` â†’ `python3.11` â†’ `python3` â†’ `python`
2. Validates version with `--version` (requires 3.11+)
3. Checks for existing venv at `~/.local/share/agent-voice-mcp/`
4. If no compatible Python found â†’ shows clear error: "Python 3.11+ required. Install with: brew install python@3.12"

---

## Future Enhancements

- **Streaming TTS playback** â€” Start playing audio before synthesis completes (for long text)
- **Continuous conversation mode** â€” Wake word detection ("Hey Eric") for always-on voice
- **Voice cloning** â€” Support custom voice training with reference audio samples
- **Emotional tags** â€” If Orpheus TTS adds Apple Silicon support, integrate for `<laugh>`, `<sigh>` etc.
- **Conversation logging** â€” Record all agent speech to a conversation transcript
- **Web UI** â€” Dashboard to manage voices, test synthesis, view conversation history
- **Larger Whisper models** â€” Optional medium/large models for higher accuracy STT
